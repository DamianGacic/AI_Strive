{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page 1. Status code: 404\n",
      "Failed to fetch page 2. Status code: 404\n",
      "Failed to fetch page 3. Status code: 404\n",
      "Failed to fetch page 4. Status code: 404\n",
      "Failed to fetch page 5. Status code: 404\n",
      "Failed to fetch page 6. Status code: 404\n",
      "Failed to fetch page 7. Status code: 404\n",
      "Failed to fetch page 8. Status code: 404\n",
      "Failed to fetch page 9. Status code: 404\n",
      "Failed to fetch page 10. Status code: 404\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Extracted Data]\n",
       "Index: []"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the extracted data\n",
    "data_list = []\n",
    "\n",
    "# Initialize the page number to 1\n",
    "page_num = 1\n",
    "\n",
    "# Loop until there are no more pages to scrape\n",
    "while True:\n",
    "    if page_num == 1:\n",
    "        page_url = \"https://www.cameo.com/browse\"\n",
    "    else:\n",
    "        page_url = f\"https://www.cameo.com/browse?page={page_num}\"\n",
    "\n",
    "    # Send an HTTP GET request to the page\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all <p> elements with the specified attributes\n",
    "        p_elements = soup.find_all(\"p\", {\"color\": \"foreground.subtle\", \"class\": \"Text-sc-1d6qffq-0 bPdxub\"})\n",
    "\n",
    "        # Extract the text within each <p> element\n",
    "        for p_element in p_elements:\n",
    "            extracted_text = p_element.text.strip()\n",
    "            data_list.append(extracted_text)\n",
    "\n",
    "        # Increment the page number for the next iteration\n",
    "        page_num += 1\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}. Status code: {response.status_code}\")\n",
    "        break  # Exit the loop if the page cannot be fetched\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\"Extracted Data\": data_list})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted Data\n",
      "0            (126)\n",
      "1            (129)\n",
      "2            (446)\n",
      "3           (1030)\n",
      "4            (189)\n",
      "..             ...\n",
      "995           (15)\n",
      "996           (24)\n",
      "997          (140)\n",
      "998           (90)\n",
      "999            (7)\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# Initialize an empty list to store the extracted data\n",
    "data_list = []\n",
    "\n",
    "# Define the number of pages to scrape\n",
    "num_pages = 100  # Change this to the number of pages you want to scrape\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_num):\n",
    "    if page_num == 1:\n",
    "        page_url = \"https://www.cameo.com/browse\"\n",
    "    else:\n",
    "        page_url = f\"https://www.cameo.com/browse?page={page_num}\"\n",
    "\n",
    "    # Send an HTTP GET request to the page\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all <p> elements with the specified attributes\n",
    "        p_elements = soup.find_all(\"p\", {\"color\": \"foreground.subtle\", \"class\": \"Text-sc-1d6qffq-0 bPdxub\"})\n",
    "\n",
    "        # Extract the text within each <p> element\n",
    "        for p_element in p_elements:\n",
    "            extracted_text = p_element.text.strip()\n",
    "            data_list.append(extracted_text)\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a list of page numbers\n",
    "page_numbers = list(range(1, num_pages + 1))\n",
    "\n",
    "# Use ThreadPoolExecutor for concurrent scraping\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(scrape_page, page_numbers)\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\"Extracted Data\": data_list})\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(126)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(129)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(446)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(1030)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(189)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>(15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>(24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>(140)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>(90)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>(7)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Extracted Data\n",
       "0            (126)\n",
       "1            (129)\n",
       "2            (446)\n",
       "3           (1030)\n",
       "4            (189)\n",
       "..             ...\n",
       "995           (15)\n",
       "996           (24)\n",
       "997          (140)\n",
       "998           (90)\n",
       "999            (7)\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Extracted Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(5131)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(82)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(431)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3484)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2199)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>(23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>(99)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>(169)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>(291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>(42)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Extracted Data\n",
       "0           (5131)\n",
       "1             (82)\n",
       "2            (431)\n",
       "3           (3484)\n",
       "4           (2199)\n",
       "..             ...\n",
       "995           (23)\n",
       "996           (99)\n",
       "997          (169)\n",
       "998          (291)\n",
       "999           (42)\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the extracted data\n",
    "data_list = []\n",
    "\n",
    "# Define the number of pages to scrape\n",
    "num_pages = 26  # Change this to the number of pages you want to scrape\n",
    "\n",
    "# Loop through the desired pages\n",
    "for page_num in range(1, num_pages + 1):\n",
    "    if page_num == 1:\n",
    "        page_url = \"https://www.cameo.com/browse\"\n",
    "    else:\n",
    "        page_url = f\"https://www.cameo.com/browse?page={page_num}\"\n",
    "\n",
    "    # Send an HTTP GET request to the page\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all <p> elements with the specified attributes\n",
    "        p_elements = soup.find_all(\"p\", {\"color\": \"foreground.subtle\", \"class\": \"Text-sc-1d6qffq-0 bPdxub\"})\n",
    "\n",
    "        # Extract the text within each <p> element\n",
    "        for p_element in p_elements:\n",
    "            extracted_text = p_element.text.strip()\n",
    "            data_list.append(extracted_text)\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\"Extracted Data\": data_list})\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Extracted Data\n",
      "0            (176)\n",
      "1             (95)\n",
      "2            (872)\n",
      "3            (837)\n",
      "4            (529)\n",
      "..             ...\n",
      "995           (70)\n",
      "996          (181)\n",
      "997           (42)\n",
      "998           (24)\n",
      "999          (124)\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import queue\n",
    "\n",
    "# Initialize a queue to store the extracted data\n",
    "data_queue = queue.Queue()\n",
    "\n",
    "# Define the number of pages to scrape\n",
    "num_pages = 26  # Change this to the number of pages you want to scrape\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(page_num):\n",
    "    if page_num == 1:\n",
    "        page_url = \"https://www.cameo.com/browse\"\n",
    "    else:\n",
    "        page_url = f\"https://www.cameo.com/browse?page={page_num}\"\n",
    "\n",
    "    # Send an HTTP GET request to the page\n",
    "    response = requests.get(page_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Find all <p> elements with the specified attributes\n",
    "        p_elements = soup.find_all(\"p\", {\"color\": \"foreground.subtle\", \"class\": \"Text-sc-1d6qffq-0 bPdxub\"})\n",
    "\n",
    "        # Extract the text within each <p> element\n",
    "        for p_element in p_elements:\n",
    "            extracted_text = p_element.text.strip()\n",
    "            data_queue.put(extracted_text)  # Use queue to maintain order\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_num}. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a list of page numbers\n",
    "page_numbers = list(range(1, num_pages + 1))\n",
    "\n",
    "# Use ThreadPoolExecutor for concurrent scraping\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    executor.map(scrape_page, page_numbers)\n",
    "\n",
    "# Collect data from the queue and maintain order\n",
    "data_list = []\n",
    "while not data_queue.empty():\n",
    "    data_list.append(data_queue.get())\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "df = pd.DataFrame({\"Extracted Data\": data_list})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [VergÃ¼tung, Title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the webpage you want to scrape\n",
    "url = 'https://www.arbeitsagentur.de/jobsuche/suche?angebotsart=1&was=Berlin&berufserfahrung=1&id=10000-1196033725-S'  # Replace with the actual URL\n",
    "\n",
    "# Send an HTTP GET request to the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Initialize lists to store scraped data\n",
    "verguetung_list = []\n",
    "title_list = []\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find and extract the content from the specified IDs\n",
    "    verguetung_element = soup.find('p', id='jobdetails-verguetung')\n",
    "    title_element = soup.find('h4', id='jobdetails-titel')\n",
    "    \n",
    "    # Check if the elements are found on the page\n",
    "    if verguetung_element and title_element:\n",
    "        verguetung_text = verguetung_element.text.strip()\n",
    "        title_text = title_element.text.strip()\n",
    "        \n",
    "        # Append the scraped data to the respective lists\n",
    "        verguetung_list.append(verguetung_text)\n",
    "        title_list.append(title_text)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "# Create a pandas DataFrame from the scraped data\n",
    "data = {'VergÃ¼tung': verguetung_list, 'Title': title_list}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame or perform further analysis\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace this URL with the URL of the webpage you want to scrape\n",
    "url = \"https://example.com\"  # Replace with your URL\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize a counter for the element IDs\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        # Construct the ID based on the current counter value\n",
    "        element_id = f\"eintrag-{counter}-index\"\n",
    "\n",
    "        # Find the element with the constructed ID\n",
    "        element = soup.find('span', {'id': element_id})\n",
    "\n",
    "        # Check if the element was found\n",
    "        if element:\n",
    "            # Extract and print the content of the element\n",
    "            print(f\"Element {element_id}: {element.text.strip()}\")\n",
    "        else:\n",
    "            # If the element is not found, break the loop\n",
    "            break\n",
    "\n",
    "        # Increment the counter for the next iteration\n",
    "        counter += 1\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this URL with the URL of the webpage you want to scrape\n",
    "url = \"https://example.com\"  # Replace with your URL\n",
    "\n",
    "# Send an HTTP GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Initialize a list to store the scraped data\n",
    "    data = []\n",
    "\n",
    "    # Initialize a counter for the element IDs\n",
    "    counter = 0\n",
    "\n",
    "    while True:\n",
    "        # Construct the ID based on the current counter value\n",
    "        element_id = f\"eintrag-{counter}-index\"\n",
    "\n",
    "        # Find the element with the constructed ID\n",
    "        element = soup.find('span', {'id': element_id})\n",
    "\n",
    "        # Check if the element was found\n",
    "        if element:\n",
    "            # Extract the content of the element\n",
    "            content = element.text.strip()\n",
    "            \n",
    "            # Append the content to the data list\n",
    "            data.append({'Element ID': element_id, 'Content': content})\n",
    "        else:\n",
    "            # If the element is not found, break the loop\n",
    "            break\n",
    "\n",
    "        # Increment the counter for the next iteration\n",
    "        counter += 1\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
