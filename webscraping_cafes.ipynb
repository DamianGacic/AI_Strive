{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/2ndhome/AI_Strive/webscraping_cafes.ipynb Cell 1\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/2ndhome/AI_Strive/webscraping_cafes.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m all_amenities \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/2ndhome/AI_Strive/webscraping_cafes.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m Url \u001b[39min\u001b[39;00m Urls:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/2ndhome/AI_Strive/webscraping_cafes.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     page \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(Url)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/2ndhome/AI_Strive/webscraping_cafes.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m0.5\u001b[39m, \u001b[39m1.4\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/2ndhome/AI_Strive/webscraping_cafes.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39;49mcontent\n\u001b[1;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:1077\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m chunk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_chunk(amt)\n\u001b[0;32m-> 1077\u001b[0m decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(\n\u001b[1;32m   1078\u001b[0m     chunk, decode_content\u001b[39m=\u001b[39;49mdecode_content, flush_decoder\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1079\u001b[0m )\n\u001b[1;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m decoded:\n\u001b[1;32m   1081\u001b[0m     \u001b[39myield\u001b[39;00m decoded\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:445\u001b[0m, in \u001b[0;36mBaseHTTPResponse._decode\u001b[0;34m(self, data, decode_content, flush_decoder)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder:\n\u001b[0;32m--> 445\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decoder\u001b[39m.\u001b[39;49mdecompress(data)\n\u001b[1;32m    446\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_decoded_content \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDECODER_ERROR_CLASSES \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py:125\u001b[0m, in \u001b[0;36mGzipDecoder.decompress\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m         ret \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_obj\u001b[39m.\u001b[39;49mdecompress(data)\n\u001b[1;32m    126\u001b[0m     \u001b[39mexcept\u001b[39;00m zlib\u001b[39m.\u001b[39merror:\n\u001b[1;32m    127\u001b[0m         previous_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#scrape cafe Names\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(10, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scraping cafe ammenities. the yelp webpages regularly change classnames, so adjust as needed\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(10, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(0.4, 2)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements with class 'css-19v1rkv'\n",
    "links = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "Urls = []\n",
    "for link in links:\n",
    "    # Extract the 'href' attribute\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Check if the href starts with \"/biz\" and only add those to the list\n",
    "    if href.startswith(\"/biz\"):\n",
    "        full_url = urljoin(\"https://www.yelp.com\", href)\n",
    "        Urls.append(full_url)\n",
    "\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "all_amenities = []\n",
    "\n",
    "for Url in Urls:\n",
    "    page = requests.get(Url)\n",
    "    x = np.random.uniform(0.5, 1.4)\n",
    "    time.sleep(x)\n",
    "    \n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Extract all elements with the specified class and store them in lists\n",
    "    left = [elem.text for elem in soup.find_all('span', class_='css-1p9ibgf')]\n",
    "    right = [elem.text for elem in soup.find_all('span', class_='css-qyp8bo')]\n",
    "\n",
    "    # Combine the lists into a single list called \"amenities\" for each URL\n",
    "    amenities = left + right\n",
    "    \n",
    "    # Append the amenities list to the overall results list\n",
    "    all_amenities.append(amenities)\n",
    "\n",
    "# Print the combined list of amenities for each URL\n",
    "for name, amenities in zip(NamesList, all_amenities):\n",
    "    print(\"Cafe Name:\", name)\n",
    "    print(\"Amenities:\", amenities)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scraping cafe ammenities. the yelp webpages regularly change classnames, so adjust as needed\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(10, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(0.4, 2)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements with class 'css-19v1rkv'\n",
    "links = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "Urls = []\n",
    "for link in links:\n",
    "    # Extract the 'href' attribute\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Check if the href starts with \"/biz\" and only add those to the list\n",
    "    if href.startswith(\"/biz\"):\n",
    "        full_url = urljoin(\"https://www.yelp.com\", href)\n",
    "        Urls.append(full_url)\n",
    "\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "all_amenities = []\n",
    "\n",
    "for Url in Urls:\n",
    "    page = requests.get(Url)\n",
    "    x = np.random.uniform(0.5, 1.4)\n",
    "    time.sleep(x)\n",
    "    \n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Extract all elements with the specified class and store them in lists\n",
    "    left = [elem.text for elem in soup.find_all('span', class_='css-1p9ibgf')]\n",
    "    right = [elem.text for elem in soup.find_all('span', class_='css-qyp8bo')]\n",
    "\n",
    "    # Combine the lists into a single list called \"amenities\" for each URL\n",
    "    amenities = left + right\n",
    "    \n",
    "    # Append the amenities list to the overall results list\n",
    "    all_amenities.append(amenities)\n",
    "\n",
    "# Print the combined list of amenities for each URL\n",
    "for name, amenities in zip(NamesList, all_amenities):\n",
    "    print(\"Cafe Name:\", name)\n",
    "    print(\"Amenities:\", amenities)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scraping cafe ammenities. the yelp webpages regularly change classnames, so adjust as needed\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(10, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(0.4, 2)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements with class 'css-19v1rkv'\n",
    "links = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "Urls = []\n",
    "for link in links:\n",
    "    # Extract the 'href' attribute\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Check if the href starts with \"/biz\" and only add those to the list\n",
    "    if href.startswith(\"/biz\"):\n",
    "        full_url = urljoin(\"https://www.yelp.com\", href)\n",
    "        Urls.append(full_url)\n",
    "\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "all_amenities = []\n",
    "\n",
    "for Url in Urls:\n",
    "    page = requests.get(Url)\n",
    "    x = np.random.uniform(0.5, 1.4)\n",
    "    time.sleep(x)\n",
    "    \n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Extract all elements with the specified class and store them in lists\n",
    "    left = [elem.text for elem in soup.find_all('span', class_='css-1p9ibgf')]\n",
    "    right = [elem.text for elem in soup.find_all('span', class_='css-qyp8bo')]\n",
    "\n",
    "    # Combine the lists into a single list called \"amenities\" for each URL\n",
    "    amenities = left + right\n",
    "    \n",
    "    # Append the amenities list to the overall results list\n",
    "    all_amenities.append(amenities)\n",
    "\n",
    "# Print the combined list of amenities for each URL\n",
    "for name, amenities in zip(NamesList, all_amenities):\n",
    "    print(\"Cafe Name:\", name)\n",
    "    print(\"Amenities:\", amenities)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a class=\"css-19v1rkv\" href=\"https://www.yelp.com/\">Yelp</a>, <a class=\"css-19v1rkv\" href=\"/search?find_desc=Restaurants&amp;find_loc=Amsterdam%2C+Noord-Holland\" role=\"link\">Restaurants</a>, <a class=\"css-19v1rkv\" href=\"/search?find_desc=cafes&amp;find_loc=Amsterdam%2C+Noord-Holland\" role=\"link\">Cafes</a>, <a class=\"css-19v1rkv\" href=\"/biz/zuivere-koffie-amsterdam-2?osq=cafes\" name=\"Zuivere Koffie\" rel=\"noopener\" role=\"link\" target=\"_blank\">Zuivere Koffie</a>, <a class=\"css-19v1rkv\" href=\"/biz/calf-en-bloom-amsterdam?osq=cafes\" name=\"Calf &amp; Bloom\" rel=\"noopener\" role=\"link\" target=\"_blank\">Calf &amp; Bloom</a>, <a class=\"css-19v1rkv\" href=\"/biz/blushing-amsterdam-3?osq=cafes\" name=\"Blushing\" rel=\"noopener\" role=\"link\" target=\"_blank\">Blushing</a>, <a class=\"css-19v1rkv\" href=\"/biz/coffee-en-coconuts-amsterdam?osq=cafes\" name=\"Coffee &amp; Coconuts\" rel=\"noopener\" role=\"link\" target=\"_blank\">Coffee &amp; Coconuts</a>, <a class=\"css-19v1rkv\" href=\"/biz/louis-amsterdam?osq=cafes\" name=\"Louis\" rel=\"noopener\" role=\"link\" target=\"_blank\">Louis</a>, <a class=\"css-19v1rkv\" href=\"/biz/bakhuys-amsterdam?osq=cafes\" name=\"Bakhuys\" rel=\"noopener\" role=\"link\" target=\"_blank\">Bakhuys</a>, <a class=\"css-19v1rkv\" href=\"/biz/bagels-en-beans-amsterdam-5?osq=cafes\" name=\"Bagels &amp; Beans\" rel=\"noopener\" role=\"link\" target=\"_blank\">Bagels &amp; Beans</a>, <a class=\"css-19v1rkv\" href=\"/biz/toastable-amsterdam?osq=cafes\" name=\"Toastable\" rel=\"noopener\" role=\"link\" target=\"_blank\">Toastable</a>, <a class=\"css-19v1rkv\" href=\"/biz/koffiehuis-de-hoek-amsterdam?osq=cafes\" name=\"Koffiehuis De Hoek\" rel=\"noopener\" role=\"link\" target=\"_blank\">Koffiehuis De Hoek</a>, <a class=\"css-19v1rkv\" href=\"/biz/brug-34-amsterdam?osq=cafes\" name=\"Brug 34\" rel=\"noopener\" role=\"link\" target=\"_blank\">Brug 34</a>]\n"
     ]
    }
   ],
   "source": [
    "#scrape cafe Names\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "print(names)\n",
    "NamesList = []\n",
    "for i in range(min(20, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CleanNamesList = NamesList[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zuivere Koffie',\n",
       " 'Calf & Bloom',\n",
       " 'Blushing',\n",
       " 'Coffee & Coconuts',\n",
       " 'Louis',\n",
       " 'Bakhuys',\n",
       " 'Bagels & Beans',\n",
       " 'Toastable',\n",
       " 'Koffiehuis De Hoek',\n",
       " 'Brug 34']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CleanNamesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping cafe ammenities. the yelp webpages regularly change classnames, so adjust as needed\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(1, 3)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(10, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=10\")\n",
    "x = np.random.uniform(0.4, 2)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements with class 'css-19v1rkv'\n",
    "links = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "Urls = []\n",
    "for link in links:\n",
    "    # Extract the 'href' attribute\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Check if the href starts with \"/biz\" and only add those to the list\n",
    "    if href.startswith(\"/biz\"):\n",
    "        full_url = urljoin(\"https://www.yelp.com\", href)\n",
    "        Urls.append(full_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.yelp.com/biz/zuivere-koffie-amsterdam-2?osq=cafes',\n",
       " 'https://www.yelp.com/biz/calf-en-bloom-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/blushing-amsterdam-3?osq=cafes',\n",
       " 'https://www.yelp.com/biz/coffee-en-coconuts-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/louis-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/bakhuys-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/bagels-en-beans-amsterdam-5?osq=cafes',\n",
       " 'https://www.yelp.com/biz/toastable-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/koffiehuis-de-hoek-amsterdam?osq=cafes',\n",
       " 'https://www.yelp.com/biz/brug-34-amsterdam?osq=cafes']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cafe Name: 4850\n",
      "Amenities: ['Takes Reservations', 'Offers Takeout', 'Accepts Debit Cards', 'Accepts Cryptocurrency']\n",
      "\n",
      "\n",
      "Cafe Name: Croissanterie C’est Magnifique\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'Free Wi-Fi', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Caffènation\n",
      "Amenities: ['Offers Takeout', 'Outdoor Seating', 'Good for Working', 'Accepts Debit Cards']\n",
      "\n",
      "\n",
      "Cafe Name: Cafe De Pels\n",
      "Amenities: ['Accepts Debit Cards', 'Outdoor Seating', 'Casual', 'No Reservations']\n",
      "\n",
      "\n",
      "Cafe Name: Koffie Academie\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'Outdoor Seating', 'Good for Working']\n",
      "\n",
      "\n",
      "Cafe Name: Frederix Roastery X Coffee Bar\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'Outdoor Seating', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Kop Van Jut\n",
      "Amenities: ['Takes Reservations', 'Accepts Debit Cards', 'No Delivery', 'No Takeout']\n",
      "\n",
      "\n",
      "Cafe Name: De Wasserette\n",
      "Amenities: ['Offers Takeout', 'Many Vegetarian Options', 'No Reservations', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Coffeebru\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'No Reservations', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Scandinavian Embassy\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'No Reservations', 'No Delivery']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#scraping cafe ammenities. the yelp webpages regularly change classnames, so adjust as needed\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "page = requests.get(\"https://www.yelp.com/search?find_desc=cafes&find_loc=amsterdam&start=30\")\n",
    "x = np.random.uniform(0.4, 0.7)\n",
    "time.sleep(x)\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find elements with the new class name \"css-19v1rkv\"\n",
    "names = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "NamesList = []\n",
    "for i in range(min(20, len(names))):  # Use min to avoid index out of range error\n",
    "    NamesList.append(names[i].text)\n",
    "\n",
    "CleanNamesList = NamesList[3:]\n",
    "\n",
    "page.content\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "# Find all <a> elements with class 'css-19v1rkv'\n",
    "links = soup.find_all('a', class_=\"css-19v1rkv\")\n",
    "\n",
    "Urls = []\n",
    "for link in links:\n",
    "    # Extract the 'href' attribute\n",
    "    href = link.get('href')\n",
    "    \n",
    "    # Check if the href starts with \"/biz\" and only add those to the list\n",
    "    if href.startswith(\"/biz\"):\n",
    "        full_url = urljoin(\"https://www.yelp.com\", href)\n",
    "        Urls.append(full_url)\n",
    "\n",
    "\n",
    "\n",
    "# List of URLs to scrape\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "all_amenities = []\n",
    "\n",
    "for Url in Urls:\n",
    "    page = requests.get(Url)\n",
    "    x = np.random.uniform(0.5, 0.9)\n",
    "    time.sleep(x)\n",
    "    \n",
    "    page.content\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Extract all elements with the specified class and store them in lists\n",
    "    left = [elem.text for elem in soup.find_all('span', class_='css-1p9ibgf')]\n",
    "    right = [elem.text for elem in soup.find_all('span', class_='css-qyp8bo')]\n",
    "\n",
    "    # Combine the lists into a single list called \"amenities\" for each URL\n",
    "    amenities = left + right\n",
    "    \n",
    "    # Append the amenities list to the overall results list\n",
    "    all_amenities.append(amenities)\n",
    "\n",
    "# Print the combined list of amenities for each URL\n",
    "for name, amenities in zip(CleanNamesList, all_amenities):\n",
    "    print(\"Cafe Name:\", name)\n",
    "    print(\"Amenities:\", amenities)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cafe Name: Grey Area Coffeeshop\n",
      "Amenities: ['Offers Takeout', 'Free Wi-Fi', 'Accepts Debit Cards', 'No Outdoor Seating']\n",
      "\n",
      "\n",
      "Cafe Name: De Bakkerswinkel\n",
      "Amenities: ['Takes Reservations', 'Offers Takeout', 'Accepts Debit Cards', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Cinema Club & Cafe\n",
      "Amenities: ['Takes Reservations', 'Accepts Debit Cards', 'No Delivery', 'No Takeout']\n",
      "\n",
      "\n",
      "Cafe Name: Cafe de Bazel\n",
      "Amenities: ['Takes Reservations', 'Offers Takeout', 'Accepts Debit Cards', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Bloem\n",
      "Amenities: ['Takes Reservations', 'Accepts Debit Cards', 'No Delivery', 'No Takeout']\n",
      "\n",
      "\n",
      "Cafe Name: Van Kerkwijk\n",
      "Amenities: ['Many Vegetarian Options', 'No Reservations', 'No Delivery', 'No Takeout']\n",
      "\n",
      "\n",
      "Cafe Name: Toast It!\n",
      "Amenities: ['Free Wi-Fi', 'TV', 'Accepts Debit Cards']\n",
      "\n",
      "\n",
      "Cafe Name: Eerlijk\n",
      "Amenities: ['Takes Reservations', 'Offers Takeout', 'Outdoor Seating', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Pluk Amsterdam\n",
      "Amenities: ['Free Wi-Fi', 'No Reservations', 'No Delivery']\n",
      "\n",
      "\n",
      "Cafe Name: Latei\n",
      "Amenities: ['Offers Takeout', 'Accepts Debit Cards', 'No Reservations', 'No Delivery']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, amenities in zip(CleanNamesList, all_amenities):\n",
    "    print(\"Cafe Name:\", name)\n",
    "    print(\"Amenities:\", amenities)\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
